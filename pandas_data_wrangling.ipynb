{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling with pandas\n",
    "\n",
    "Data preparation is often the most important part of data analysis. Much of the programming work in data analysis and modeling is spent on data preparation: loading, cleaning, transforming, and rearranging. Sometimes the way that data is stored in files or databases is not the way you need it for data processing. Fortunately, pandas along with the Python standard library provide you with a high level, flexible, and high-performance set of core manipulations and algorithms to enable you to wrangle data into the right form without much trouble.\n",
    "\n",
    "\n",
    "### MovieLens 1M Data Set\n",
    "\n",
    "GroupLens Research provides a number of [collections of movie ratings data](http://grouplens.org/datasets/movielens/) collected from users of MovieLens. The data provide movie ratings, movie metadata (genres and year), and demographic data about the users (age, zip code, gender, and occupation).\n",
    "\n",
    "Download data set [The MovieLens 1M Data Set](http://files.grouplens.org/datasets/movielens/ml-1m.zip) (ml-1m.zip, 5.64MB). It contains about 1M+ ratings collected from 6K+ users on about 4K movies (check exact numbers!). It's spread across 3 tables: ratings, user information, and movie information. \n",
    "\n",
    "### ZIP codes\n",
    "\n",
    "Also download <a href=\"ftp://ftp.census.gov/econ2013/CBP_CSV/zbp13totals.zip\">Complete ZIP Code Totals</a> (725KB) at The Census Bureau's website.\n",
    "\n",
    "Create a folder called \"movies\", copy both downloaded files in, and unpack them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files\n",
    "\n",
    "Reading files and writing to files are two very common operations while working with data. Most often you will be using read_csv and to_csv functions, as you have already been doing during the introductory pandas lecture. These two functions can operate with different type of delimiter-separated files, which is the prevailing format among data-based files. At the end of the notebook we will also learn how to read and write excel files. \n",
    "\n",
    "### Reading .csv files\n",
    "\n",
    "* text files, values are comma-separated (csv = comma-separated files)\n",
    "* standard format for storing tabular data\n",
    "* values in row delimeted usually with ',' (but sometimes other characters, like a tab)\n",
    "* Use quotes \"\", when you wish to hide delimiters in text. \n",
    "* methods [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) and [`to_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unames = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
    "rnames = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "mnames = ['movie_id', 'title', 'genres']\n",
    "\n",
    "users = pd.read_csv('movies/ml-1m/users.dat', sep='::', header=None, engine='python', encoding='latin1', names=unames)\n",
    "ratings = pd.read_csv('movies/ml-1m/ratings.dat', sep='::', header=None, names=rnames, encoding='latin1', engine='python')\n",
    "movies = pd.read_csv('movies/ml-1m/movies.dat', sep='::', header=None, names=mnames, encoding='latin1', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables `users`, `ratings`, `movies` are dataframes. Remember, the main pandas structure is dataframe, which contains table data (two dimensional). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimensions of the data frames gives as an immediate answer about the number of users, ratings, and movies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that everything loaded correctly by looking at the first/last few rows of each data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Python's slice index produces the same result as \"movies.head()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Merging data</h3>\n",
    "\n",
    "It is much easier to work with all of the data merged together into a single table.\n",
    "\n",
    "Data contained in pandas objects can be combined together in a number of built-in ways. These two are the most common:\n",
    "<ul>\n",
    "   <li>[pandas.merge](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) connects rows in DataFrames based on one or more keys. This will be familiar to users of SQL or other relational databases, as it implements database <em>join</em> operations. </li>\n",
    "   <li>[pandas.concat](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.concat.html) glues or stacks together objects along an axis.</li>\n",
    "</ul>\n",
    "\n",
    "Read more: [Merge, join, and concatenate](href=http://pandas.pydata.org/pandas-docs/stable/merging.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now combine `users`, `ratings`, and `movies` dataframes. We will have to use `merge`, because we are not just concatenating, but combining them relatively to some column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first merge ratings with users... (the common key is *user_id*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ratings with users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then we merge the resulting DataFrame with movies data... (the common key is *movie_id*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df with movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Transformation</h3>\n",
    "\n",
    "Filtering, cleaning and other transformations are important operations in data preparation. Removing duplicates, handling missing values, replacing values, mapping values are examples of such operations.\n",
    "\n",
    "<hr>\n",
    "The values in the column <em>age</em> are not as expected. It's time to read more about the data in the README file.\n",
    "\n",
    "USERS FILE DESCRIPTION\n",
    "\n",
    "Age is chosen from the following ranges:\n",
    "*  1:  \"Under 18\"\n",
    "* 18:  \"18-24\"\n",
    "* 25:  \"25-34\"\n",
    "* 35:  \"35-44\"\n",
    "* 45:  \"45-49\"\n",
    "* 50:  \"50-55\"\n",
    "* 56:  \"56+\"\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which values are present in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python dictionary (<key>-<value> pairs) in order to replace <keys> with <values>.\n",
    "age_group = {\n",
    "    1:  \"Under 18\",\n",
    "    18:  \"18-24\",\n",
    "    25:  \"25-34\",\n",
    "    35:  \"35-44\",\n",
    "    45:  \"45-49\",\n",
    "    50:  \"50-55\",\n",
    "    56:  \"56+\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the column <em>occupation</em> are not as expected either. Let's see the README file once again.\n",
    "\n",
    "USERS FILE DESCRIPTION\n",
    "\n",
    "Occupation is chosen from the following choices:\n",
    "*  0:  \"other\" or not specified\n",
    "*  1:  \"academic/educator\"\n",
    "*  2:  \"artist\"\n",
    "*  3:  \"clerical/admin\"\n",
    "*  4:  \"college/grad student\"\n",
    "*  5:  \"customer service\"\n",
    "*  6:  \"doctor/health care\"\n",
    "*  7:  \"executive/managerial\"\n",
    "*  8:  \"farmer\"\n",
    "*  9:  \"homemaker\"\n",
    "* 10:  \"K-12 student\"\n",
    "* 11:  \"lawyer\"\n",
    "* 12:  \"programmer\"\n",
    "* 13:  \"retired\"\n",
    "* 14:  \"sales/marketing\"\n",
    "* 15:  \"scientist\"\n",
    "* 16:  \"self-employed\"\n",
    "* 17:  \"technician/engineer\"\n",
    "* 18:  \"tradesman/craftsman\"\n",
    "* 19:  \"unemployed\"\n",
    "* 20:  \"writer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether all these values are present in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation = {\n",
    "    0:  \"other\",\n",
    "    1:  \"academic/educator\",\n",
    "    2:  \"artist\",\n",
    "    3:  \"clerical/admin\",\n",
    "    4:  \"college/grad student\",\n",
    "    5:  \"customer service\",\n",
    "    6:  \"doctor/health care\",\n",
    "    7:  \"executive/managerial\",\n",
    "    8:  \"farmer\",\n",
    "    9:  \"homemaker\",\n",
    "    10:  \"K-12 student\",\n",
    "    11:  \"lawyer\",\n",
    "    12:  \"programmer\",\n",
    "    13:  \"retired\",\n",
    "    14:  \"sales/marketing\",\n",
    "    15:  \"scientist\",\n",
    "    16:  \"self-employed\",\n",
    "    17:  \"technician/engineer\",\n",
    "    18:  \"tradesman/craftsman\",\n",
    "    19:  \"unemployed\",\n",
    "    20:  \"writer\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZIP codes\n",
    "\n",
    "ZIP codes are useful, however, we are not interested in so much details: it would be more informative for us to know from which US states the users come from. This information is contained in the ZIP codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check the type of 'zip' column in the current data frame: is it a number or a string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the CSV file into DataFrame object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip_path = os.path.expanduser('movies/zbp13totals.txt')\n",
    "zip_path = 'movies/zbp13totals.txt'\n",
    "zip_codes = pd.read_csv(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative: specify format in read_csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_codes = pd.read_csv(zip_path, converters={\"zip\":str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both files have a common column 'zip', so we can merge them. The only column that we need from zip_codes is 'stabbr' (state abbreviation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>More on merging data</h3>\n",
    "\n",
    "The *how* argument to merge specifies how to determine which keys are to be included in the resulting table. Here is a summary of the how options and their SQL equivalent names:\n",
    "\n",
    "Merge method | SQL Join Name    | Description\n",
    "-------------|------------------|--------------\n",
    "left         |LEFT OUTER JOIN   | Use keys from left frame only\n",
    "right        |RIGHT OUTER JOIN  | Use keys from right frame only\n",
    "outer        |FULL OUTER JOIN   | Use union of keys from both frames\n",
    "inner        |INNER JOIN        | Use intersection of keys from both frames\n",
    "\n",
    "Read more: [Merge, join, and concatenate](href=http://pandas.pydata.org/pandas-docs/stable/merging.html).\n",
    "\n",
    "Nice vizualization of various types of joins (taken from Ravjot Singh post at <a href=\"https://medium.com/swlh/merging-dataframes-with-pandas-pd-merge-7764c7e2d46d\">medium.com</a>): <img src=\"https://miro.medium.com/max/700/1*9eH1_7VbTZPZd9jBiGIyNA.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data with zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rename 'stabbr' to 'state'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Handling Missing Data</h3>\n",
    "\n",
    "Missing data is common in most data analysis applications. In pandas, missing data is represented by the floating point value <font face=\"Times New Roman, Times, serif\"><b>NaN</b></font>.\n",
    "\n",
    "One of the goals of pandas was to make working with missing data as painless as possible. The following table briefly introduces the most common methods for this purpose.\n",
    "\n",
    "\n",
    "Command      | Description\n",
    "-------------|--------------\n",
    "dropna       | Filter axis labels based on whether values for each label have missing data, with varying thresholds for how much missing data to tolerate.\n",
    "fillna       | Fill in missing data with some value or using an interpolation method such as 'ffill' or 'bfill'.\n",
    "isnull       | Return like-type object containing boolean values indicating which values are missing.\n",
    "\n",
    "Read more: [Working with missing data](http://pandas.pydata.org/pandas-docs/stable/missing_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we will simply remove the rows with false ZIP code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any missing values in state variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some of such rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data frame now only contains rows with correctly written ZIP codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is not correct now, as we dropped some rows. We have to reset it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index is correct now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transforming Data Using a Function or Mapping</h3>\n",
    "\n",
    "For many data sets, you may wish to perform some transformation based on the values in an array, Series, or column in a DataFrame.\n",
    "\n",
    "In the present case, we want to make the following transformations:\n",
    "<ul>\n",
    "    <li>Convert epoch times in 'timestamp' column into a readable (and usable) format.</li>\n",
    "    <li>Convert each letter in 'genres' column to lower case.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `pd.to_datetime` is used to convert values into datetime object. Function contains many arguments used for date formatting, but we can simply call it with default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many combinations of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Display only first 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each letter in 'genres' to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of `groupby`\n",
    "\n",
    "* Use groupby mechanics when you need to split the data along some dimension, do something with each split and then combine the results. \n",
    "* Examples:\n",
    "    * aggregated statistics: group sums, mean, sizes, counts, etc.\n",
    "    * group-specific transformations: standardization within groups, fillin missing values within groups with a value derived from each group, discarding group according to some group criteria (e.g. movies with less than 10 ratings), etc.\n",
    "\n",
    "We will demonstrate how to compute average rating for each occupation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "\n",
    "### Saving data to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is ready now. Let's store it into a csv file. Note: the size of the file could be ~100MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame into CSV file. \n",
    "df.to_csv('movie_lens_1M.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
